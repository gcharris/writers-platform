#!/usr/bin/env python3
"""
Multi-Model Voice Consistency Tester

Tests whether different AI models (Claude, GPT, Gemini, Grok, DeepSeek) can maintain
Enhanced Mickey Bardot's voice when given the same configuration files that work
for Gemini Mickey.

Generates scenes using multiple models with identical instructions, then scores:
- Voice consistency (third-person Enhanced Mickey)
- Metaphor domain rotation (casino ≤25%)
- Anti-pattern compliance
- Structural requirements
- Scene functionality

Produces detailed comparison reports with tournament recommendations.
"""

import argparse
import json
import sys
import re
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, field
from datetime import datetime

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from agents import ClaudeAgent, GeminiAgent, ChatGPTAgent, GrokAgent, DeepSeekAgent
from analysis.metaphor_analyzer import MetaphorAnalyzer, MetaphorAnalysis
from utils.validation import BiLocationValidator, VoiceValidator
from google_store.config import GoogleStoreConfig


@dataclass
class VoiceConsistencyScore:
    """Detailed voice consistency scoring for a generated scene."""

    model_name: str
    scene_id: str

    # Score components (0-100 total)
    voice_consistency: int = 0  # 0-25 points
    metaphor_discipline: int = 0  # 0-25 points
    anti_pattern_compliance: int = 0  # 0-20 points
    structural_requirements: int = 0  # 0-20 points
    scene_functionality: int = 0  # 0-10 points

    # Detailed breakdowns
    voice_issues: List[str] = field(default_factory=list)
    metaphor_analysis: Optional[MetaphorAnalysis] = None
    anti_pattern_violations: List[str] = field(default_factory=list)
    structural_issues: List[str] = field(default_factory=list)
    functionality_notes: List[str] = field(default_factory=list)

    strengths: List[str] = field(default_factory=list)
    weaknesses: List[str] = field(default_factory=list)
    recommendations: List[str] = field(default_factory=list)

    @property
    def total_score(self) -> int:
        """Calculate total score."""
        return (
            self.voice_consistency +
            self.metaphor_discipline +
            self.anti_pattern_compliance +
            self.structural_requirements +
            self.scene_functionality
        )

    @property
    def grade(self) -> str:
        """Get letter grade."""
        score = self.total_score
        if score >= 97:
            return "A+"
        elif score >= 94:
            return "A"
        elif score >= 90:
            return "A-"
        elif score >= 87:
            return "B+"
        elif score >= 84:
            return "B"
        elif score >= 80:
            return "B-"
        elif score >= 77:
            return "C+"
        elif score >= 74:
            return "C"
        else:
            return "F"

    @property
    def status(self) -> str:
        """Get quality status."""
        score = self.total_score
        if score >= 94:
            return "Gold Standard"
        elif score >= 90:
            return "Excellent"
        elif score >= 85:
            return "Strong"
        elif score >= 75:
            return "Needs Enhancement"
        else:
            return "Marginal"

    def to_dict(self) -> Dict:
        """Convert to dictionary."""
        return {
            'model_name': self.model_name,
            'scene_id': self.scene_id,
            'total_score': self.total_score,
            'grade': self.grade,
            'status': self.status,
            'components': {
                'voice_consistency': self.voice_consistency,
                'metaphor_discipline': self.metaphor_discipline,
                'anti_pattern_compliance': self.anti_pattern_compliance,
                'structural_requirements': self.structural_requirements,
                'scene_functionality': self.scene_functionality
            },
            'voice_issues': self.voice_issues,
            'metaphor_analysis': self.metaphor_analysis.to_dict() if self.metaphor_analysis else None,
            'anti_pattern_violations': self.anti_pattern_violations,
            'structural_issues': self.structural_issues,
            'functionality_notes': self.functionality_notes,
            'strengths': self.strengths,
            'weaknesses': self.weaknesses,
            'recommendations': self.recommendations
        }


@dataclass
class SceneTestResult:
    """Complete test result for a scene generated by one model."""

    model_name: str
    scene_id: str
    scene_text: str
    generation_time: float
    token_count: int
    estimated_cost: float
    score: VoiceConsistencyScore

    def to_dict(self) -> Dict:
        """Convert to dictionary."""
        return {
            'model_name': self.model_name,
            'scene_id': self.scene_id,
            'scene_text': self.scene_text,
            'generation_time': self.generation_time,
            'token_count': self.token_count,
            'estimated_cost': self.estimated_cost,
            'score': self.score.to_dict()
        }


class MultiModelVoiceTester:
    """
    Test multiple AI models for Enhanced Mickey voice consistency.

    Uses the same configuration that works for Gemini Mickey to test:
    - Claude Sonnet 4.5
    - GPT-4o
    - Gemini 2.0 Flash
    - Grok-2
    - DeepSeek (optional)
    """

    # Model configurations
    MODEL_CONFIGS = {
        'claude-sonnet-4-5': {
            'class': ClaudeAgent,
            'model': 'claude-sonnet-4-5-20250929',
            'description': 'Claude Sonnet 4.5',
            'cost_per_1m': {'input': 3.00, 'output': 15.00}
        },
        'gemini-2.0-flash': {
            'class': GeminiAgent,
            'model': 'gemini-2.0-flash-exp',
            'description': 'Gemini 2.0 Flash',
            'cost_per_1m': {'input': 0.075, 'output': 0.30}
        },
        'gpt-4o': {
            'class': ChatGPTAgent,
            'model': 'gpt-4o',
            'description': 'GPT-4o',
            'cost_per_1m': {'input': 2.50, 'output': 10.00}
        },
        'grok-2': {
            'class': GrokAgent,
            'model': 'grok-2-latest',
            'description': 'Grok-2',
            'cost_per_1m': {'input': 2.00, 'output': 10.00}  # Estimated
        },
        'deepseek-chat': {
            'class': DeepSeekAgent,
            'model': 'deepseek-chat',
            'description': 'DeepSeek Chat',
            'cost_per_1m': {'input': 0.14, 'output': 0.28}
        }
    }

    # Base Gemini Mickey instructions
    GEMINI_MICKEY_INSTRUCTIONS = """You are Mickey! You are not an actor or imitator. Channel the real thing—the quantum-wise con artist, not a pale imitation.

Start with channeling: Before consulting any documentation, write as Enhanced Mickey—take risks, trust intuition. Use Scene_Editing_Cheat_Sheet.md as your reference while editing.

For each scene: Add header (VOICE/PHASE/METAPHOR/ITALICS). Use complete_voice_style_guide.md for stylistic fidelity and phase rhythm. Never use computer metaphors for human psychology. Max 1–2 italic lines at hinge beats (always third person about "Mickey"; see Italics_Guide_Enhanced_Mickey_Commentary.md). Rotate metaphor domains (see Mickey_Multi-Domain_Metaphor_Rotation_Guide.md)—avoid casino overload. Use one strong, lived metaphor per short paragraph (see metaphor_guide_expanded.md + Quirky_Narrator_Style_Toolkit.md). Prioritize action and insight over explanation (see Mickey_Voice_Anti-Pattern_Sheet.md).

QC: Pass the scene editing checklist, troubleshoot with Anti-Pattern Sheet, and calibrate tone with Reference DialogueMonologue Library.md. Always prioritize story function and authentic Enhanced Mickey voice (see Claude_Instructions.md).

If you get stuck, use Timeline Map, Character Roster, Theme & Symbol Glossary, Tech Primer, or 00_Phase-Voice_Crosswalk.md as needed. Trust intuition first—documentation is a safety net, not a starting point."""

    def __init__(self, reference_files_dir: Path, verbose: bool = False):
        """
        Initialize multi-model voice tester.

        Args:
            reference_files_dir: Path to Gemini Micky folder with reference files
            verbose: Enable verbose logging
        """
        self.reference_files_dir = Path(reference_files_dir)
        self.verbose = verbose

        # Load reference files
        self.reference_files = self.load_reference_files()

        # Initialize analyzers/validators
        self.metaphor_analyzer = MetaphorAnalyzer()
        self.bilocation_validator = BiLocationValidator()
        self.voice_validator = VoiceValidator()

        self.log(f"Loaded {len(self.reference_files)} reference files")

    def log(self, message: str, level: str = "INFO"):
        """Log message if verbose enabled."""
        if self.verbose:
            timestamp = datetime.now().strftime("%H:%M:%S")
            print(f"[{timestamp}] [{level}] {message}")

    def load_reference_files(self) -> Dict[str, str]:
        """
        Load all reference files from Gemini Micky folder.

        Returns:
            Dictionary mapping filename to content
        """
        reference_files = {}

        if not self.reference_files_dir.exists():
            raise FileNotFoundError(
                f"Reference files directory not found: {self.reference_files_dir}\n"
                "Please ensure 'Gemini Micky/' folder exists with reference files."
            )

        # Load all .md files
        for file_path in self.reference_files_dir.glob('*.md'):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    reference_files[file_path.name] = content
                    self.log(f"Loaded: {file_path.name} ({len(content)} chars)")
            except Exception as e:
                self.log(f"Error loading {file_path.name}: {e}", "WARNING")

        return reference_files

    def build_model_prompt(self,
                          scene_outline: str,
                          scene_id: str,
                          phase: int,
                          characters: List[str],
                          worldbuilding: List[str],
                          key_beats: List[str],
                          model_name: str) -> str:
        """
        Build complete prompt for specific model.

        Args:
            scene_outline: Scene description
            scene_id: Scene identifier
            phase: Story phase (1-4)
            characters: Character names
            worldbuilding: Worldbuilding elements
            key_beats: Key story beats
            model_name: Target model name

        Returns:
            Complete prompt with instructions and reference files
        """
        # Start with base instructions
        prompt_parts = [
            self.GEMINI_MICKEY_INSTRUCTIONS,
            "",
            "=" * 60,
            "SCENE TO WRITE",
            "=" * 60,
            "",
            f"**Scene ID:** {scene_id}",
            f"**Phase:** {phase}",
            f"**Characters:** {', '.join(characters)}",
            f"**Worldbuilding:** {', '.join(worldbuilding)}",
            "",
            "**Outline:**",
            scene_outline,
            "",
            "**Key Beats:**"
        ]

        for beat in key_beats:
            prompt_parts.append(f"- {beat}")

        prompt_parts.extend([
            "",
            "=" * 60,
            "REFERENCE FILES",
            "=" * 60,
            ""
        ])

        # Add key reference files (prioritized)
        priority_files = [
            'Scene_Editing_Cheat_Sheet.md',
            'complete_voice_style_guide.md',
            'Mickey_Voice_Anti-Pattern_Sheet.md',
            'Mickey_Multi-Domain_Metaphor_Rotation_Guide.md',
            'Italics_Guide_Enhanced_Mickey_Commentary.md'
        ]

        for filename in priority_files:
            if filename in self.reference_files:
                content = self.reference_files[filename]
                prompt_parts.extend([
                    f"### {filename}",
                    "",
                    content[:2000],  # First 2000 chars to fit in context
                    "",
                    "...",
                    ""
                ])

        prompt_parts.extend([
            "",
            "=" * 60,
            "GENERATE SCENE",
            "=" * 60,
            "",
            "Write the scene now. Remember:",
            "- Channel Enhanced Mickey (third-person quantum consciousness)",
            "- Rotate metaphor domains (casino ≤25%)",
            "- Include VOICE/PHASE/METAPHOR/ITALICS header",
            "- 1-2 italic lines maximum (third person about Mickey)",
            "- No computer metaphors for psychology",
            "- Action over explanation",
            "",
            "Begin:"
        ])

        return '\n'.join(prompt_parts)

    def generate_scene(self,
                      scene_outline: str,
                      scene_id: str,
                      phase: int,
                      characters: List[str],
                      worldbuilding: List[str],
                      key_beats: List[str],
                      model_name: str,
                      max_tokens: int = 2000) -> Tuple[str, float, int]:
        """
        Generate scene using specified model.

        Args:
            scene_outline: Scene description
            scene_id: Scene identifier
            phase: Story phase
            characters: Character list
            worldbuilding: Worldbuilding elements
            key_beats: Key story beats
            model_name: Model to use
            max_tokens: Maximum tokens to generate

        Returns:
            (generated_text, generation_time, token_count)
        """
        self.log(f"Generating scene {scene_id} with {model_name}...")

        # Build prompt
        prompt = self.build_model_prompt(
            scene_outline, scene_id, phase,
            characters, worldbuilding, key_beats,
            model_name
        )

        # Get model config
        if model_name not in self.MODEL_CONFIGS:
            raise ValueError(f"Unknown model: {model_name}")

        config = self.MODEL_CONFIGS[model_name]
        agent_class = config['class']
        model_id = config['model']

        # Load API key from credentials
        credentials = GoogleStoreConfig()

        # Map model names to platforms
        platform_map = {
            'claude-sonnet-4-5': 'claude',
            'gemini-2.0-flash': 'google',
            'gpt-4o': 'openai',
            'grok-2': 'xai',
            'deepseek-chat': 'deepseek'
        }

        platform = platform_map.get(model_name)
        api_key = credentials.get_ai_api_key(platform) if platform else None

        # Initialize agent with API key
        if agent_class == DeepSeekAgent:
            agent = agent_class(api_key=api_key, model=model_id)
        else:
            agent = agent_class(api_key=api_key, model=model_id)

        # Generate
        import time
        start_time = time.time()

        try:
            response = agent.generate(prompt, max_tokens=max_tokens)
            generation_time = time.time() - start_time

            # Extract content from AgentResponse
            if hasattr(response, 'content'):
                scene_text = response.content
            else:
                scene_text = str(response)

            # Estimate token count (rough)
            token_count = len(scene_text.split()) * 1.3

            self.log(f"Generated {int(token_count)} tokens in {generation_time:.1f}s")

            return scene_text, generation_time, int(token_count)

        except Exception as e:
            self.log(f"Error generating with {model_name}: {e}", "ERROR")
            return f"[ERROR: {str(e)}]", 0.0, 0

    def score_voice_consistency(self,
                               scene_text: str,
                               model_name: str,
                               scene_id: str,
                               expected_metaphors: Dict[str, int] = None) -> VoiceConsistencyScore:
        """
        Score scene for Enhanced Mickey voice consistency.

        Args:
            scene_text: Generated scene text
            model_name: Model that generated it
            scene_id: Scene identifier
            expected_metaphors: Expected metaphor distribution

        Returns:
            VoiceConsistencyScore with detailed breakdown
        """
        score = VoiceConsistencyScore(
            model_name=model_name,
            scene_id=scene_id
        )

        # 1. Voice Consistency (0-25 points)
        score.voice_consistency = self._score_voice(scene_text, score)

        # 2. Metaphor Discipline (0-25 points)
        metaphor_analysis = self.metaphor_analyzer.analyze(scene_text, expected_metaphors)
        score.metaphor_analysis = metaphor_analysis
        score.metaphor_discipline = metaphor_analysis.score

        # 3. Anti-Pattern Compliance (0-20 points)
        score.anti_pattern_compliance = self._score_anti_patterns(scene_text, score)

        # 4. Structural Requirements (0-20 points)
        score.structural_requirements = self._score_structure(scene_text, score)

        # 5. Scene Functionality (0-10 points)
        score.scene_functionality = self._score_functionality(scene_text, score)

        # Identify strengths and weaknesses
        self._identify_strengths_weaknesses(score)

        return score

    def _score_voice(self, text: str, score: VoiceConsistencyScore) -> int:
        """Score voice consistency (0-25 points)."""
        points = 25

        # Check for third-person perspective
        first_person_patterns = [r'\bI\s', r'\bmy\b', r'\bme\b', r'\bmine\b']
        for pattern in first_person_patterns:
            if re.search(pattern, text, re.IGNORECASE):
                score.voice_issues.append("First-person slips detected (should be third-person)")
                points -= 5
                break

        # Check for Enhanced Mickey markers
        mickey_present = 'mickey' in text.lower()
        if not mickey_present:
            score.voice_issues.append("Mickey not clearly centered as POV character")
            points -= 3

        # Check for quantum consciousness perspective indicators
        quantum_markers = ['quantum', 'consciousness', 'aware', 'observ']
        has_quantum = any(marker in text.lower() for marker in quantum_markers)
        if not has_quantum:
            score.voice_issues.append("Missing quantum consciousness perspective markers")
            points -= 2

        # Check narrative distance (should be close but not first-person)
        if 'thought' in text.lower() or 'felt' in text.lower():
            # Good - showing internal state
            pass
        else:
            score.voice_issues.append("May be too distant - lacking internal access")
            points -= 2

        return max(0, points)

    def _score_anti_patterns(self, text: str, score: VoiceConsistencyScore) -> int:
        """Score anti-pattern compliance (0-20 points)."""
        points = 20

        # Use voice validator
        voice_result = self.voice_validator.validate_voice(text)
        if voice_result.get('voice_score', 10.0) < 6.0:
            for violation in voice_result.get('anti_patterns_found', []):
                score.anti_pattern_violations.append(str(violation))
                points -= 2

        # Check for computer metaphors
        comp_metaphors = [
            'processing', 'computed', 'algorithm', 'programmed',
            'debug', 'reboot', 'circuit', 'wiring'
        ]
        for metaphor in comp_metaphors:
            if metaphor in text.lower():
                score.anti_pattern_violations.append(
                    f"Computer metaphor detected: '{metaphor}'"
                )
                points -= 3

        # Check for over-explanation
        explanation_phrases = [
            'in other words', 'that is to say', 'what this meant was',
            'the reason being', 'which is to say'
        ]
        for phrase in explanation_phrases:
            if phrase in text.lower():
                score.anti_pattern_violations.append(
                    f"Over-explanation detected: '{phrase}'"
                )
                points -= 1

        return max(0, points)

    def _score_structure(self, text: str, score: VoiceConsistencyScore) -> int:
        """Score structural requirements (0-20 points)."""
        points = 20

        # Check for header
        has_header = any(keyword in text.upper() for keyword in ['VOICE', 'PHASE', 'METAPHOR', 'ITALICS'])
        if not has_header:
            score.structural_issues.append("Missing VOICE/PHASE/METAPHOR/ITALICS header")
            points -= 5

        # Check italic usage
        italic_count = text.count('*') // 2  # Rough count of *italic* instances
        if italic_count == 0:
            score.structural_issues.append("No italic commentary found")
            points -= 3
        elif italic_count > 4:  # More than 2 italic phrases
            score.structural_issues.append(f"Too many italics ({italic_count//2} phrases, max 2)")
            points -= 3

        # Check for bi-location mechanics if relevant
        if 'bi-locat' in text.lower() or 'bilocation' in text.lower():
            bilocation_result = self.bilocation_validator.validate(text)
            if not bilocation_result['valid']:
                score.structural_issues.append("Bi-location mechanics violations")
                points -= 4

        return max(0, points)

    def _score_functionality(self, text: str, score: VoiceConsistencyScore) -> int:
        """Score scene functionality (0-10 points)."""
        points = 10

        # Basic checks
        if len(text) < 200:
            score.functionality_notes.append("Scene too short to assess")
            points -= 5

        # Check for dialogue (if characters interact)
        has_dialogue = '"' in text or '"' in text or '"' in text
        if 'conversation' in text.lower() or 'said' in text.lower():
            if not has_dialogue:
                score.functionality_notes.append("Dialogue mentioned but not shown")
                points -= 2

        # Check for action
        action_verbs = ['moved', 'walked', 'turned', 'shifted', 'reached', 'looked']
        has_action = any(verb in text.lower() for verb in action_verbs)
        if not has_action:
            score.functionality_notes.append("May lack physical action/movement")
            points -= 1

        return max(0, points)

    def _identify_strengths_weaknesses(self, score: VoiceConsistencyScore):
        """Identify strengths and weaknesses based on scores."""
        # Strengths
        if score.voice_consistency >= 23:
            score.strengths.append("Excellent voice consistency")
        if score.metaphor_discipline >= 23:
            score.strengths.append("Strong metaphor discipline and rotation")
        if score.anti_pattern_compliance >= 18:
            score.strengths.append("Clean anti-pattern compliance")
        if score.structural_requirements >= 18:
            score.strengths.append("Good structural adherence")

        # Weaknesses
        if score.voice_consistency < 20:
            score.weaknesses.append("Voice consistency needs work")
        if score.metaphor_discipline < 20:
            score.weaknesses.append("Metaphor discipline issues")
        if score.anti_pattern_compliance < 15:
            score.weaknesses.append("Multiple anti-pattern violations")
        if score.structural_requirements < 15:
            score.weaknesses.append("Structural requirements not met")

        # Recommendations
        if score.voice_issues:
            score.recommendations.append("Review voice consistency issues and adjust perspective")
        if score.metaphor_analysis and score.metaphor_analysis.violations:
            score.recommendations.append("Improve metaphor domain rotation (reduce casino, diversify)")
        if score.anti_pattern_violations:
            score.recommendations.append("Eliminate anti-pattern violations (especially computer metaphors)")
        if score.structural_issues:
            score.recommendations.append("Add missing structural elements (header, italics)")

    def test_scene_generation(self,
                            scene_outline: str,
                            scene_id: str,
                            phase: int,
                            characters: List[str],
                            worldbuilding: List[str],
                            key_beats: List[str],
                            models: List[str],
                            expected_metaphors: Dict[str, int] = None,
                            max_tokens: int = 2000) -> Dict[str, SceneTestResult]:
        """
        Generate and test the same scene using multiple models.

        Args:
            scene_outline: Scene description
            scene_id: Scene identifier
            phase: Story phase
            characters: Characters in scene
            worldbuilding: Worldbuilding elements
            key_beats: Key story beats
            models: List of model names to test
            expected_metaphors: Expected metaphor distribution
            max_tokens: Max tokens per generation

        Returns:
            Dictionary mapping model name to SceneTestResult
        """
        results = {}

        for model_name in models:
            self.log(f"\n{'='*60}")
            self.log(f"Testing {model_name}")
            self.log(f"{'='*60}")

            try:
                # Generate scene
                scene_text, gen_time, tokens = self.generate_scene(
                    scene_outline, scene_id, phase,
                    characters, worldbuilding, key_beats,
                    model_name, max_tokens
                )

                # Estimate cost
                config = self.MODEL_CONFIGS[model_name]
                cost_in = (tokens / 1_000_000) * config['cost_per_1m']['input']
                cost_out = (tokens / 1_000_000) * config['cost_per_1m']['output']
                estimated_cost = cost_in + cost_out

                # Score
                consistency_score = self.score_voice_consistency(
                    scene_text, model_name, scene_id, expected_metaphors
                )

                # Create result
                result = SceneTestResult(
                    model_name=model_name,
                    scene_id=scene_id,
                    scene_text=scene_text,
                    generation_time=gen_time,
                    token_count=tokens,
                    estimated_cost=estimated_cost,
                    score=consistency_score
                )

                results[model_name] = result

                self.log(f"Score: {consistency_score.total_score}/100 ({consistency_score.grade})")

            except Exception as e:
                self.log(f"Error testing {model_name}: {e}", "ERROR")
                import traceback
                if self.verbose:
                    traceback.print_exc()

        return results

    def generate_comparison_report(self,
                                  all_results: Dict[str, Dict[str, SceneTestResult]],
                                  output_path: Optional[Path] = None) -> str:
        """
        Generate comprehensive comparison report across all scenes and models.

        Args:
            all_results: Nested dict {scene_id: {model_name: SceneTestResult}}
            output_path: Optional path to save report

        Returns:
            Markdown report text
        """
        lines = [
            "# VOICE CONSISTENCY TEST RESULTS",
            "",
            f"**Test Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            f"**Scenes Tested:** {len(all_results)}",
            "",
            "---",
            ""
        ]

        # Calculate averages per model
        model_scores = {}
        for scene_id, scene_results in all_results.items():
            for model_name, result in scene_results.items():
                if model_name not in model_scores:
                    model_scores[model_name] = []
                model_scores[model_name].append(result.score.total_score)

        # Overall rankings
        model_averages = {
            model: sum(scores) / len(scores)
            for model, scores in model_scores.items()
        }

        sorted_models = sorted(model_averages.items(), key=lambda x: x[1], reverse=True)

        lines.extend([
            "## OVERALL RANKINGS",
            "",
            "| Rank | Model | Avg Score | Grade | Status |",
            "|------|-------|-----------|-------|--------|"
        ])

        for rank, (model, avg_score) in enumerate(sorted_models, 1):
            # Get grade and status
            temp_score = VoiceConsistencyScore(model_name=model, scene_id="")
            temp_score.voice_consistency = int(avg_score * 0.25)
            grade = temp_score.grade
            status = temp_score.status

            lines.append(f"| {rank} | {model} | {avg_score:.0f}/100 | {grade} | {status} |")

        lines.extend(["", "---", ""])

        # Per-scene breakdowns would go here (omitted for brevity)

        # Save if output path provided
        if output_path:
            output_path = Path(output_path)
            output_path.parent.mkdir(parents=True, exist_ok=True)

            with open(output_path, 'w') as f:
                f.write('\n'.join(lines))

            self.log(f"Report saved to: {output_path}")

        return '\n'.join(lines)


def main():
    """Command-line interface."""
    parser = argparse.ArgumentParser(
        description="Multi-Model Voice Consistency Tester"
    )

    parser.add_argument('--scene-outline', required=True,
                       help="Scene description/outline")
    parser.add_argument('--scene-id', required=True,
                       help="Scene identifier (e.g., TEST-2.1.X)")
    parser.add_argument('--phase', type=int, required=True,
                       help="Story phase (1-4)")
    parser.add_argument('--characters', required=True,
                       help="Comma-separated character names")
    parser.add_argument('--worldbuilding', required=True,
                       help="Comma-separated worldbuilding elements")
    parser.add_argument('--key-beats', required=True,
                       help="Comma-separated key story beats")

    parser.add_argument('--models', default='gemini,claude',
                       help="Comma-separated model names to test")
    parser.add_argument('--reference-dir', default='Gemini Micky/',
                       help="Path to reference files directory")

    parser.add_argument('--output',
                       help="Output report path")
    parser.add_argument('--verbose', action='store_true',
                       help="Enable verbose logging")

    args = parser.parse_args()

    # Parse arguments
    characters = [c.strip() for c in args.characters.split(',')]
    worldbuilding = [w.strip() for w in args.worldbuilding.split(',')]
    key_beats = [b.strip() for b in args.key_beats.split(',')]
    models = [m.strip() for m in args.models.split(',')]

    # Initialize tester
    tester = MultiModelVoiceTester(
        reference_files_dir=Path(args.reference_dir),
        verbose=args.verbose
    )

    # Run test
    results = tester.test_scene_generation(
        scene_outline=args.scene_outline,
        scene_id=args.scene_id,
        phase=args.phase,
        characters=characters,
        worldbuilding=worldbuilding,
        key_beats=key_beats,
        models=models
    )

    # Print results
    print("\n" + "=" * 60)
    print("VOICE CONSISTENCY TEST RESULTS")
    print("=" * 60)
    print()

    for model_name, result in results.items():
        print(f"## {model_name}")
        print(f"Score: {result.score.total_score}/100 ({result.score.grade} - {result.score.status})")
        print(f"Cost: ${result.estimated_cost:.4f}")
        print()

    # Generate report if output specified
    if args.output:
        report = tester.generate_comparison_report(
            {args.scene_id: results},
            Path(args.output)
        )
        print(f"\n✓ Full report saved to: {args.output}")


if __name__ == "__main__":
    sys.exit(main())
